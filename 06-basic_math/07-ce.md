# 크로스엔트로피 (Cross Entropy)

$$\begin{aligned}
H(P,Q)&=-\mathbb{E}_{\text{x}\sim{P}}\Big[\log{Q}(x)\Big] \\
&=-\sum_{x\in\mathcal{X}}{P(x)\log{Q(x)}}
\end{aligned}$$

$$\begin{gathered}
\mathcal{D}=\{x_i,y_i\}_{i=1}^N \\
\\
\mathcal{L}(\theta)=H(P,P_\theta), \\
\text{where }P_\theta=P(\text{y}|\text{x};\theta). \\
\\
\hat{\theta}=\underset{\theta\in\Theta}{\text{argmin }}\mathcal{L}(\theta) \\
\theta\leftarrow\theta-\alpha\nabla_\theta{\mathcal{L}(\theta)}
\end{gathered}$$

## with KLD

$$\begin{aligned}\text{KL}(P||Q)&=-\mathbb{E}_{\text{x}\sim{P}}\bigg[\log{\frac{Q(x)}{P(x)}}\bigg] \\
&=-\sum_{x\in\mathcal{X}}P(x)\log{\frac{Q(x)}{P(x)}} \\
&=-\sum_{x\in\mathcal{X}}\Big(P(x)\log{Q(x)}-P(x)\log{P(x)}\Big) \\
&=-\sum_{x\in\mathcal{X}}{P(x)\log{Q(x)}}+\sum_{x\in\mathcal{X}}{P(x)\log{P(x)}} \\
&=H(P,Q)-H(P)
\end{aligned}$$

## with MLE

$$\begin{gathered}
\mathcal{L}(\theta)=-\mathbb{E}_{x\sim{P(\text{x})}}\Big[\log{P(x;\theta)}\Big] \\
H(P,Q)=-\mathbb{E}_{\text{x}\sim{P}}\Big[\log{Q}(x)\Big] \\
\\
Q(x)\coloneqq{P(x;\theta)} \\
\text{Thus, }H(P,Q)=H(P,P_\theta)=\mathcal{L}(\theta).
\end{gathered}$$

## Derivative of KLD equals to CE's one

$$\begin{aligned}
\nabla_\theta\text{KL}(P||P_\theta)&=\nabla_\theta{H(P,P_\theta)}-\nabla_\theta{H(P)} \\
&=\nabla_\theta{H(P,P_\theta)} \\
&=\nabla_\theta\mathcal{L}(\theta)
\end{aligned}$$
